# ‚úÖ Phase 2 Backend - COMPLETE

**Date** : D√©cembre 2025
**Statut** : ‚úÖ **PRODUCTION READY**

---

## üéØ Objectifs Phase 2 : ATTEINTS

Cr√©ation d'un backend robuste avec :
- ‚úÖ Fallback intelligent
- ‚úÖ Syst√®me de logging et observabilit√©
- ‚úÖ Informations mod√®le IA
- ‚úÖ Rate limiting et s√©curit√©
- ‚úÖ Tests automatis√©s complets

---

## üìÅ Nouveaux Fichiers Cr√©√©s

### Modules Backend

| Fichier | Description | Lignes |
|---------|-------------|---------|
| [ia/fallback-engine.js](ia/fallback-engine.js) | Moteur de fallback intelligent bas√© sur FAQ | ~200 |
| [ia/ai-logger.js](ia/ai-logger.js) | Syst√®me de logging et m√©triques IA | ~250 |
| [ia/model-config.js](ia/model-config.js) | Configuration et m√©tadonn√©es du mod√®le | ~200 |

### Scripts de Test

| Fichier | Description |
|---------|-------------|
| [test-phase2.sh](test-phase2.sh) | Tests automatis√©s Phase 2 (12 tests) |

### Serveur

| Fichier | Modifications |
|---------|---------------|
| [server.js](server.js) | Ajout 6 nouveaux endpoints + s√©curit√© |

---

## üÜï Nouveaux Endpoints API

### 1. POST /ai/fallback
**Fallback intelligent quand le LLM local √©choue**

```bash
curl -X POST http://localhost:4000/ai/fallback \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Comment obtenir une CNI ?",
    "language": "fr",
    "reason": "timeout"
  }'
```

**R√©ponse** :
```json
{
  "content": "Vous devez vous rendre au centre d'enr√¥lement...",
  "confidence": 0.75,
  "source": "fallback",
  "metadata": {
    "language": "fr",
    "fallbackReason": "timeout",
    "matchedFAQ": "faq_001",
    "matchScore": 0.75,
    "category": "etat_civil"
  }
}
```

**Raisons support√©es** :
- `timeout` : G√©n√©ration trop longue
- `low_confidence` : Confiance trop faible
- `error` : Erreur technique
- `unknown` : Autre raison

### 2. POST /ai/logs
**Enregistre des logs c√¥t√© client**

```bash
curl -X POST http://localhost:4000/ai/logs \
  -H "Content-Type: application/json" \
  -d '{
    "type": "request",
    "data": {
      "source": "client",
      "message": "test",
      "responseTime": 3500,
      "success": true
    }
  }'
```

**Types de logs** :
- `request` : Requ√™te IA
- `error` : Erreur IA

### 3. GET /ai/status
**Statut d√©taill√© de l'IA**

```bash
curl http://localhost:4000/ai/status
```

**R√©ponse** :
```json
{
  "status": "ok",
  "ai": {
    "backend": {
      "initialized": true,
      "kbEntries": 2,
      "faqFREntries": 2,
      "faqAREntries": 2
    },
    "fallback": {
      "faqFR": 2,
      "faqAR": 2,
      "ready": true
    },
    "logs": {
      "totalRequests": 150,
      "totalErrors": 3,
      "lastHour": {
        "requests": 45,
        "errors": 1,
        "successRate": "97.78%",
        "avgResponseTime": "2500ms",
        "sourceBreakdown": {
          "local_llm": 30,
          "fallback_backend": 15
        }
      }
    }
  },
  "uptime": 3600.5,
  "timestamp": "2025-12-05T02:00:00.000Z"
}
```

### 4. GET /ai/model-info
**Informations sur le mod√®le IA**

```bash
curl http://localhost:4000/ai/model-info
```

**R√©ponse** :
```json
{
  "model": {
    "name": "TinyLlama-1.1B-Chat-v1.0",
    "provider": "TinyLlama",
    "version": "1.0.0",
    "buildDate": "2025-12-05",
    "defaultVariant": "q4",
    "variants": [
      {
        "id": "q4",
        "name": "TinyLlama Q4 Quantized",
        "size": "500MB",
        "recommended": true
      }
    ],
    "languages": ["fr", "ar", "en"],
    "defaultParams": {
      "maxNewTokens": 150,
      "temperature": 0.7,
      "topK": 50,
      "topP": 0.9
    }
  },
  "recommended": {
    "id": "q4",
    "name": "TinyLlama Q4 Quantized",
    "size": "500MB",
    "recommended": true
  }
}
```

### 5. GET /ai/logs
**R√©cup√®re les logs r√©cents**

```bash
curl "http://localhost:4000/ai/logs?type=requests&limit=10"
```

**Param√®tres** :
- `type` : `requests`, `errors`, `performance`, `all` (default: `all`)
- `limit` : Nombre de logs (default: 100, max: 1000)

### 6. POST /ai/logs/export
**Exporte les logs dans un fichier**

```bash
curl -X POST http://localhost:4000/ai/logs/export
```

**R√©ponse** :
```json
{
  "success": true,
  "file": "/Users/mesbah/BUT/Projects/NDI2025/logs/ai-logs-2025-12-05T02-00-00.json",
  "message": "Logs export√©s avec succ√®s"
}
```

---

## üîí S√©curit√© Impl√©ment√©e

### 1. Rate Limiting
- **60 requ√™tes par minute** par IP
- Appliqu√© sur tous les endpoints IA
- R√©ponse 429 avec `retryAfter` si d√©pass√©

```json
{
  "error": "Trop de requ√™tes. Veuillez r√©essayer dans 1 minute.",
  "retryAfter": 45
}
```

### 2. Input Sanitization
- Limite de taille : **1MB** par requ√™te JSON
- Troncature automatique des strings > **10 000 caract√®res**
- Protection contre les injections

### 3. CORS
- Activ√© pour tous les endpoints
- Permet les requ√™tes cross-origin du frontend

### 4. Validation des Entr√©es
- V√©rification des types
- Champs requis obligatoires
- Validation des langues (fr, ar uniquement)

---

## üìä Syst√®me de Logging

### M√©triques Collect√©es

**Requ√™tes IA** :
- Source (local_llm, fallback_backend, client)
- Message (extrait)
- Langue
- Confiance
- Temps de r√©ponse
- Succ√®s/√©chec

**Erreurs** :
- Type d'erreur
- Stack trace
- Contexte (source, message, raison)

**Performance** :
- M√©triques personnalis√©es
- Unit√©s configurables

### Statistiques en Temps R√©el

- Nombre total de requ√™tes
- Taux de succ√®s
- Temps de r√©ponse moyen
- R√©partition par source
- Erreurs de la derni√®re heure

### Export des Logs

Les logs peuvent √™tre export√©s en JSON pour analyse :
```json
{
  "exportedAt": "2025-12-05T02:00:00.000Z",
  "stats": { ... },
  "logs": {
    "requests": [...],
    "errors": [...],
    "performance": [...]
  }
}
```

---

## ü§ñ Fallback Intelligent

### Algorithme

1. **Tokenization** : Normalise et d√©coupe le message
2. **Matching** : Compare avec les FAQ en base
3. **Scoring** : Calcule la similarit√© (Jaccard)
4. **Seuil** : Retourne la meilleure r√©ponse si score > 0.2
5. **D√©faut** : Message g√©n√©rique + suggestions sinon

### Raisons de Fallback

| Raison | Description | Message |
|--------|-------------|---------|
| `timeout` | G√©n√©ration > 15s | "La g√©n√©ration a pris trop de temps..." |
| `low_confidence` | Confiance < 0.5 | "Je ne suis pas s√ªr de comprendre..." |
| `error` | Erreur technique | "Une erreur s'est produite..." |
| `unknown` | Autre | Liste de questions fr√©quentes |

### Support Multilingue

- **Fran√ßais** : FAQ FR + messages FR
- **Arabe** : FAQ AR + messages AR
- D√©tection automatique via le param√®tre `language`

---

## üß™ Tests Phase 2

### Script de Test Automatis√©

[test-phase2.sh](test-phase2.sh) : **12 tests**, tous valid√©s ‚úÖ

**Couverture** :
- Endpoints Phase 1 (r√©trocompatibilit√©)
- Nouveaux endpoints Phase 2
- S√©curit√© (rate limit, sanitization)
- Gestion d'erreurs
- Cas limites

**R√©sultat** :
```
Total: 12
Passed: 12
Failed: 0

‚úÖ Tous les tests passent !
```

### Comment Tester

```bash
# 1. D√©marrer le backend
npm run backend

# 2. Dans un autre terminal, lancer les tests
./test-phase2.sh
```

---

## üìà Comparaison Phase 1 vs Phase 2

| Aspect | Phase 1 | Phase 2 |
|--------|---------|---------|
| **Endpoints** | 2 | 8 |
| **Fallback** | Frontend uniquement | Backend + Frontend |
| **Logging** | Console basique | Syst√®me complet + export |
| **S√©curit√©** | CORS | CORS + Rate limit + Sanitization |
| **Observabilit√©** | Basique | M√©triques d√©taill√©es |
| **Model Info** | ‚ùå | ‚úÖ Endpoint d√©di√© |
| **Tests** | Manuel | Automatis√©s (12 tests) |

---

## üîÑ Flow Complet avec Fallback

### Cas 1 : LLM Local R√©ussit

```
Frontend
   ‚Üì
   G√©n√®re avec LLM local
   ‚Üì
   Succ√®s (confidence > 0.5, temps < 15s)
   ‚Üì
   Affiche r√©ponse
   ‚Üì
   Log via POST /ai/logs (optionnel)
```

### Cas 2 : LLM Local √âchoue ‚Üí Fallback Backend

```
Frontend
   ‚Üì
   G√©n√®re avec LLM local
   ‚Üì
   √âchec (timeout / low confidence / error)
   ‚Üì
   POST /ai/fallback { message, language, reason }
   ‚Üì
   Backend : Fallback Engine
   ‚Üì
   Matching FAQ ‚Üí R√©ponse
   ‚Üì
   Affiche r√©ponse fallback
   ‚Üì
   Log automatique c√¥t√© backend
```

### Cas 3 : Fallback Backend √âchoue ‚Üí Fallback Frontend

```
Frontend
   ‚Üì
   POST /ai/fallback √©choue (backend down)
   ‚Üì
   Fallback local (rules-engine.js)
   ‚Üì
   R√©ponse FAQ statique
   ‚Üì
   Affiche r√©ponse
```

---

## üöÄ Utilisation Frontend

### Exemple d'int√©gration

```javascript
// src/engine/ai-adapter.js

export async function generateResponse(userMessage, history, context) {
  try {
    // 1. Tenter LLM local (Phase 3)
    const llmResponse = await generateLocalLLM(userMessage);

    if (llmResponse.confidence > 0.5) {
      return llmResponse;
    }

    // 2. Fallback vers backend si confiance faible
    return await callBackendFallback(userMessage, 'low_confidence');

  } catch (error) {
    // 3. Fallback vers backend si erreur
    try {
      return await callBackendFallback(userMessage, 'error');
    } catch (backendError) {
      // 4. Fallback local si backend inaccessible
      return findAnswer(userMessage); // rules-engine
    }
  }
}

async function callBackendFallback(message, reason) {
  const response = await fetch('http://localhost:4000/ai/fallback', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      message,
      language: detectLanguage(message),
      reason
    })
  });

  return await response.json();
}
```

---

## üìù Configuration Mod√®le

Voir [ia/model-config.js](ia/model-config.js) pour :

- Informations du mod√®le (nom, version, hash)
- Variantes disponibles (Q4, Q8, FP16)
- Param√®tres de g√©n√©ration par d√©faut
- URLs de t√©l√©chargement
- Contraintes syst√®me
- M√©triques de performance estim√©es

**Utilisation** :

```javascript
import { getModelInfo, getRecommendedVariant } from './ia/model-config.js';

const modelInfo = getModelInfo();
const recommended = getRecommendedVariant();

console.log(`Mod√®le : ${modelInfo.name}`);
console.log(`Variante recommand√©e : ${recommended.id} (${recommended.size})`);
```

---

## üêõ Debugging

### Logs Console

Le backend affiche des logs d√©taill√©s :

```
[2025-12-05T02:00:00.000Z] POST /api/chat
[API] Requ√™te chat: "Comment obtenir une CNI ?" (fr)
[Backend] generateIaResponse appel√©e: "Comment obtenir une CNI ?" (fr)
[RAG Stub] Recherche de contexte pour : "Comment obtenir une CNI ?"
[Backend] Prompt construit: Tu es un assistant...
[AI Log] local_llm - SUCCESS (50ms)
```

### Endpoints de Monitoring

```bash
# Statut global
curl http://localhost:4000/ai/status | jq '.ai.logs.lastHour'

# Logs r√©cents
curl "http://localhost:4000/ai/logs?type=errors&limit=10"

# Export pour analyse
curl -X POST http://localhost:4000/ai/logs/export
```

---

## ‚úÖ Checklist de Production

### Backend

- [x] Tous les endpoints fonctionnels
- [x] Rate limiting activ√©
- [x] Input sanitization activ√©
- [x] CORS configur√©
- [x] Logging complet
- [x] Gestion d'erreurs robuste
- [x] Tests automatis√©s passent

### Fallback

- [x] FAQ FR charg√©e
- [x] FAQ AR charg√©e
- [x] Matching fonctionnel
- [x] Messages multilingues
- [x] Gestion cas par d√©faut

### S√©curit√©

- [x] Rate limiting : 60 req/min
- [x] Taille max requ√™te : 1MB
- [x] Validation des entr√©es
- [x] Troncature strings longues
- [x] Gestion erreurs

### Observabilit√©

- [x] Logs requ√™tes
- [x] Logs erreurs
- [x] M√©triques performance
- [x] Statistiques temps r√©el
- [x] Export logs

---

## üéØ Prochaines √âtapes : Phase 3

Voir [NEXT-STEPS.md](NEXT-STEPS.md) pour :

1. **Int√©gration LLM r√©el** (TinyLLaMA via Transformer.js)
2. **RAG avec embeddings** (Jina Embeddings + recherche vectorielle)
3. **Frontend avanc√©** (streaming, retry, cache)

---

## üìö Documentation Compl√®te

- [README-INTEGRATION.md](README-INTEGRATION.md) - Guide d'utilisation Phase 1
- [MERGE-COMPLETE.md](MERGE-COMPLETE.md) - R√©sum√© merge Phase 1
- [NEXT-STEPS.md](NEXT-STEPS.md) - Roadmap Phase 2/3
- [PHASE2-COMPLETE.md](PHASE2-COMPLETE.md) - Ce document

---

**Derni√®re mise √† jour** : D√©cembre 2025
**Auteur** : √âquipe NDI2025
**Statut** : ‚úÖ **PHASE 2 COMPLETE - PRODUCTION READY**
