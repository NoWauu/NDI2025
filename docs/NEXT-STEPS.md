# üöÄ Prochaines √âtapes - Phase 2 & 3

**Statut Phase 1** : ‚úÖ Complete
**√Ä faire** : Phase 2 (LLM) + Phase 3 (RAG)

---

## üìã Roadmap

```
‚úÖ Phase 1 : Merge Frontend/Backend      [DONE]
   ‚îú‚îÄ Backend ES Modules
   ‚îú‚îÄ Serveur HTTP
   ‚îú‚îÄ API /api/chat
   ‚îî‚îÄ Frontend connect√©

‚¨ú Phase 2 : Int√©gration LLM R√©el        [TODO]
   ‚îú‚îÄ Installer Transformer.js
   ‚îú‚îÄ Charger TinyLLaMA
   ‚îú‚îÄ G√©n√©rer vraies r√©ponses
   ‚îî‚îÄ Optimiser temps de r√©ponse

‚¨ú Phase 3 : RAG avec Embeddings         [TODO]
   ‚îú‚îÄ Jina Embeddings
   ‚îú‚îÄ Vectorisation KB
   ‚îú‚îÄ Recherche similarit√©
   ‚îî‚îÄ Top-K documents
```

---

## üéØ Phase 2 : Int√©gration LLM R√©el

### Objectif

Remplacer le stub qui retourne le prompt par un **vrai LLM** qui g√©n√®re des r√©ponses.

### √âtapes D√©taill√©es

#### 1Ô∏è‚É£ Installer les d√©pendances

```bash
npm install @xenova/transformers
```

#### 2Ô∏è‚É£ Modifier `ia/llm.js`

```javascript
import { pipeline } from '@xenova/transformers';

let llmPipeline = null;

/**
 * Initialise le mod√®le LLM
 */
export async function initLLM() {
  console.log('[LLM] Chargement de TinyLLaMA...');

  llmPipeline = await pipeline(
    'text-generation',
    'TinyLlama/TinyLlama-1.1B-Chat-v1.0',
    {
      quantized: true,
      revision: 'main'
    }
  );

  console.log('[LLM] TinyLLaMA charg√©');
}

/**
 * G√©n√®re une r√©ponse √† partir d'un prompt
 */
export async function generateLLMResponse(prompt) {
  if (!llmPipeline) {
    throw new Error('LLM not initialized');
  }

  const result = await llmPipeline(prompt, {
    max_new_tokens: 150,
    temperature: 0.7,
    top_k: 50,
    top_p: 0.9,
    do_sample: true,
    return_full_text: false
  });

  return result[0].generated_text.trim();
}
```

#### 3Ô∏è‚É£ Modifier `ia/app.js`

```javascript
import { generateLLMResponse, initLLM } from './llm.js';

export async function initBackend() {
  // ... code existant ...

  // Charger le LLM
  await initLLM();

  isInitialized = true;
}

export async function generateIaResponse({ message, language, history }) {
  // ... code existant jusqu'√† buildPrompt ...

  // NOUVEAU : G√©n√©rer vraie r√©ponse au lieu de retourner le prompt
  const llmResponse = await generateLLMResponse(prompt);

  return {
    content: llmResponse,  // ‚Üê R√©ponse g√©n√©r√©e, pas le prompt
    confidence: 0.75,
    source: 'ai',
    metadata: {
      language,
      model: 'TinyLLaMA-1.1B',
      kbEntriesUsed: kbSnippets.length,
      faqEntriesUsed: faqSnippets.length,
      ragEnabled: false
    }
  };
}
```

#### 4Ô∏è‚É£ Tester

```bash
# Red√©marrer le backend
npm run backend

# Tester
curl -X POST http://localhost:4000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Comment obtenir une CNI ?", "language": "fr"}'
```

**R√©sultat attendu** : Une vraie r√©ponse en fran√ßais, pas le prompt

### ‚ö†Ô∏è Points d'Attention Phase 2

1. **Temps de chargement** : Le mod√®le TinyLLaMA (~500MB) peut prendre 30-60 secondes √† charger au d√©marrage
   - Solution : Ajouter un indicateur de chargement

2. **Temps de g√©n√©ration** : ~2-5 secondes par r√©ponse
   - Solution : Streaming si possible, sinon timeout √† 15s

3. **M√©moire** : TinyLLaMA n√©cessite ~2GB RAM
   - Solution : Monitorer avec `process.memoryUsage()`

4. **Qualit√© des r√©ponses** : TinyLLaMA peut halluciner
   - Solution : Affiner le prompt, ajouter des instructions claires

---

## üîç Phase 3 : RAG avec Embeddings

### Objectif

Am√©liorer la qualit√© des r√©ponses en **s√©lectionnant les documents les plus pertinents** via recherche vectorielle.

### √âtapes D√©taill√©es

#### 1Ô∏è‚É£ Installer les d√©pendances

```bash
npm install @xenova/transformers  # (d√©j√† fait en Phase 2)
```

#### 2Ô∏è‚É£ Cr√©er `ia/embeddings.js`

```javascript
import { pipeline } from '@xenova/transformers';

let embeddingPipeline = null;

export async function initEmbeddings() {
  console.log('[Embeddings] Chargement Jina...');

  embeddingPipeline = await pipeline(
    'feature-extraction',
    'Xenova/jina-embeddings-v2-small-en',
    { quantized: true }
  );

  console.log('[Embeddings] Jina charg√©');
}

export async function getEmbedding(text) {
  const result = await embeddingPipeline(text, {
    pooling: 'mean',
    normalize: true
  });

  return Array.from(result.data);
}

export function cosineSimilarity(vec1, vec2) {
  let dotProduct = 0;
  let norm1 = 0;
  let norm2 = 0;

  for (let i = 0; i < vec1.length; i++) {
    dotProduct += vec1[i] * vec2[i];
    norm1 += vec1[i] * vec1[i];
    norm2 += vec2[i] * vec2[i];
  }

  return dotProduct / (Math.sqrt(norm1) * Math.sqrt(norm2));
}
```

#### 3Ô∏è‚É£ Modifier `ia/rag.js`

```javascript
import { getEmbedding, cosineSimilarity } from './embeddings.js';

// Cache des embeddings
let kbEmbeddings = [];
let faqEmbeddings = [];

export async function indexDocuments(kbData, faqData) {
  console.log('[RAG] Indexation des documents...');

  // Vectoriser KB
  for (const kb of kbData) {
    const text = `${kb.title} ${kb.body}`;
    const embedding = await getEmbedding(text);
    kbEmbeddings.push({ id: kb.id, embedding, doc: kb });
  }

  // Vectoriser FAQ
  for (const faq of faqData) {
    const text = `${faq.question} ${faq.answer}`;
    const embedding = await getEmbedding(text);
    faqEmbeddings.push({ id: faq.id, embedding, doc: faq });
  }

  console.log(`[RAG] ${kbEmbeddings.length} KB + ${faqEmbeddings.length} FAQ index√©s`);
}

export async function selectRagContext(question, topK = 3) {
  console.log(`[RAG] Recherche pour : "${question}"`);

  // Vectoriser la question
  const queryEmbedding = await getEmbedding(question);

  // Calculer similarit√©s KB
  const kbScores = kbEmbeddings.map(entry => ({
    doc: entry.doc,
    score: cosineSimilarity(queryEmbedding, entry.embedding)
  }));

  // Calculer similarit√©s FAQ
  const faqScores = faqEmbeddings.map(entry => ({
    doc: entry.doc,
    score: cosineSimilarity(queryEmbedding, entry.embedding)
  }));

  // Trier et prendre top-K
  kbScores.sort((a, b) => b.score - a.score);
  faqScores.sort((a, b) => b.score - a.score);

  return {
    kbSnippets: kbScores.slice(0, topK).map(s => s.doc),
    faqSnippets: faqScores.slice(0, topK).map(s => s.doc)
  };
}
```

#### 4Ô∏è‚É£ Modifier `ia/app.js`

```javascript
import { indexDocuments } from './rag.js';

export async function initBackend() {
  // ... charger KB + FAQ ...

  // NOUVEAU : Indexer pour RAG
  await indexDocuments(kbData, [...faqDataFR, ...faqDataAR]);

  isInitialized = true;
}

export async function generateIaResponse({ message, language, history }) {
  // REMPLACER le code qui prenait juste les 2 premiers
  // const kbSnippets = kbData.filter(...).slice(0, 2);
  // const faqSnippets = (...).slice(0, 2);

  // PAR recherche RAG
  const ragContext = await selectRagContext(message, 3);
  const kbSnippets = ragContext.kbSnippets.filter(kb => kb.lang === language);
  const faqSnippets = ragContext.faqSnippets;

  // ... reste du code inchang√© ...
}
```

#### 5Ô∏è‚É£ Tester

```bash
# Red√©marrer le backend (indexation au d√©marrage)
npm run backend

# Tester avec une question
curl -X POST http://localhost:4000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "passeport biom√©trique", "language": "fr"}'
```

**R√©sultat attendu** : Le contexte devrait contenir les documents sur le passeport, pas la CNI

---

## üìä Comparaison Phase 1 vs Phase 2 vs Phase 3

| Aspect | Phase 1 | Phase 2 | Phase 3 |
|--------|---------|---------|---------|
| **R√©ponse** | Prompt construit | Texte g√©n√©r√© LLM | Texte g√©n√©r√© + contexte pertinent |
| **Contexte** | 2 premiers docs | 2 premiers docs | Top-3 documents similaires |
| **Temps** | ~50ms | ~2-5s | ~3-7s |
| **Qualit√©** | N/A | Moyenne | Haute |
| **Pertinence** | Faible | Faible | Haute |

---

## üß™ Tests √† Faire

### Phase 2

- [ ] Chargement mod√®le TinyLLaMA
- [ ] G√©n√©ration de r√©ponse simple
- [ ] G√©n√©ration avec historique
- [ ] Timeout si g√©n√©ration trop longue
- [ ] Qualit√© des r√©ponses FR/AR
- [ ] M√©moire utilis√©e
- [ ] Temps de r√©ponse moyen

### Phase 3

- [ ] Indexation KB + FAQ
- [ ] Recherche similarit√©
- [ ] Top-K documents corrects
- [ ] Pertinence contexte vs question
- [ ] Performance avec grande KB (100+ docs)
- [ ] Cache embeddings

---

## üéØ Priorit√©s

### Must Have (Nuit de l'Info)

1. ‚úÖ Phase 1 : Merge frontend/backend
2. ‚¨ú Phase 2 : LLM g√©n√®re vraies r√©ponses
3. ‚¨ú Interface web fonctionnelle
4. ‚¨ú Mode offline (fallback rules)

### Nice to Have

1. ‚¨ú Phase 3 : RAG avec embeddings
2. ‚¨ú Service Worker PWA
3. ‚¨ú Support complet arabe
4. ‚¨ú Optimisations performance

---

## ‚è±Ô∏è Estimation Temps

| Phase | T√¢che | Temps Estim√© |
|-------|-------|--------------|
| 2 | Installer + Configurer Transformer.js | 30 min |
| 2 | Int√©grer TinyLLaMA | 1h |
| 2 | Tests + Debugging | 1h |
| 2 | **Total Phase 2** | **2h30** |
| 3 | Embeddings Jina | 1h |
| 3 | RAG recherche vectorielle | 1h30 |
| 3 | Tests + Optimisation | 30 min |
| 3 | **Total Phase 3** | **3h** |

**Total estim√© Phase 2 + 3** : ~5h30

---

## üìö Ressources

### Documentation

- [Transformer.js Docs](https://huggingface.co/docs/transformers.js)
- [TinyLLaMA Model](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
- [Jina Embeddings](https://huggingface.co/jinaai/jina-embeddings-v2-small-en)

### Alternatives LLM

Si TinyLLaMA trop lourd :
- Qwen2.5-0.5B (~300MB, plus rapide)
- Phi-2 (~1.5GB, meilleure qualit√©)

### Alternatives Embeddings

Si Jina trop lourd :
- all-MiniLM-L6-v2 (~23MB, plus rapide)
- BGE-small-en (~33MB, bon compromis)

---

## ‚úÖ Checklist Avant Production

### Phase 2

- [ ] LLM charge sans erreur
- [ ] G√©n√©ration < 10 secondes
- [ ] Pas de hallucinations graves
- [ ] M√©moire < 3GB
- [ ] Logs clairs
- [ ] Gestion timeout
- [ ] Fallback si erreur LLM

### Phase 3

- [ ] Indexation au d√©marrage
- [ ] Top-K pertinents
- [ ] Temps indexation acceptable
- [ ] Cache embeddings
- [ ] Pas de fuites m√©moire

---

**Derni√®re mise √† jour** : D√©cembre 2025
**Prochaine action** : Impl√©menter Phase 2 (LLM)
